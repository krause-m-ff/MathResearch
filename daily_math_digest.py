"""
Math Daily Digest â€” æ•°å­¦ãƒ»å°‚é–€æƒ…å ± æ—¥æ¬¡é…ä¿¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

arXiv, Quanta Magazine, æ•°å­¦ãƒ–ãƒ­ã‚°, YouTubeæ•°å­¦ãƒãƒ£ãƒ³ãƒãƒ«ç­‰ã‹ã‚‰
æ¯æ—¥ã®æ–°ç€è¨˜äº‹ã‚’å–å¾—ã—ã€Discord Webhook ã§é…ä¿¡ã™ã‚‹ã€‚

Phase A: feedparserã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­– + Discordãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒªãƒˆãƒ©ã‚¤
Phase B: AIè¦ç´„(Gemini) + åˆ†é‡ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
"""

import os
import sys
import json
import re
import time
import logging
from datetime import datetime, timedelta, timezone
from html import unescape

import feedparser
import requests
import yaml
from bs4 import BeautifulSoup
from dateutil import parser as date_parser

# â”€â”€ ãƒ­ã‚°è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
log = logging.getLogger(__name__)

# â”€â”€ å®šæ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.yaml")
MAX_DESCRIPTION_LENGTH = 200
DISCORD_EMBED_LIMIT = 6000      # Discord embed åˆè¨ˆæ–‡å­—æ•°åˆ¶é™
DISCORD_FIELD_LIMIT = 1024       # Discord embed field value ä¸Šé™
MAX_EMBEDS_PER_MESSAGE = 10      # Discord ã¯1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æœ€å¤§10 embed
FEED_READ_DEADLINE_SEC = 15      # ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ã®ãƒãƒ¼ãƒ‰ãƒ‡ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³
FEED_MAX_BYTES = 1 * 1024 * 1024 # ãƒ•ã‚£ãƒ¼ãƒ‰æœ€å¤§ã‚µã‚¤ã‚º (1MB)

# Gemini API
GEMINI_MODELS = [  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é †ï¼ˆã‚¯ã‚©ãƒ¼ã‚¿è¶…éæ™‚ã«æ¬¡ã‚’è©¦è¡Œï¼‰
    "gemini-2.5-flash-lite",    # æœ€ã‚‚å®‰å®šï¼ˆã‚¯ã‚©ãƒ¼ã‚¿æ®‹é‡ãŒå¤šã„ï¼‰
    "gemini-2.0-flash",
    "gemini-2.0-flash-lite",
]
GEMINI_BATCH_SIZE = 8            # 1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šã®è¨˜äº‹æ•°ï¼ˆå¤§ãã™ãã‚‹ã¨ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã«æŠµè§¦ï¼‰


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è¨­å®šèª­ã¿è¾¼ã¿
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_config(path: str = CONFIG_PATH) -> dict:
    """config.yaml ã‚’èª­ã¿è¾¼ã‚€ã€‚"""
    with open(path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    # ç’°å¢ƒå¤‰æ•°å±•é–‹
    webhook = cfg.get("discord", {}).get("webhook_url", "")
    if webhook.startswith("${") and webhook.endswith("}"):
        env_key = webhook[2:-1]
        cfg["discord"]["webhook_url"] = os.environ.get(env_key, "")

    return cfg


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RSS ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾— (Phase A-1: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–æ¸ˆã¿)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def clean_html(raw_html: str) -> str:
    """HTML ã‚¿ã‚°ã‚’é™¤å»ã—ã€ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™ã€‚"""
    if not raw_html:
        return ""
    soup = BeautifulSoup(raw_html, "html.parser")
    text = soup.get_text(separator=" ", strip=True)
    text = unescape(text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def extract_category(entry: dict, feed_name: str) -> str:
    """è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒª/åˆ†é‡ã‚’æ¨å®šã™ã‚‹ã€‚"""
    tags = entry.get("tags", [])
    if tags:
        terms = [t.get("term", "") for t in tags if t.get("term")]
        arxiv_cats = [t for t in terms if re.match(r"^[a-z]+\.[A-Z]{2}$", t)]
        if arxiv_cats:
            return ", ".join(arxiv_cats[:3])
        if terms:
            return terms[0][:50]
    return ""


def truncate(text: str, max_len: int = MAX_DESCRIPTION_LENGTH) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šé•·ã§åˆ‡ã‚Šè©°ã‚ã‚‹ã€‚"""
    if len(text) <= max_len:
        return text
    return text[: max_len - 1] + "â€¦"


def _fetch_feed_content(url: str, name: str) -> bytes:
    """requests + iter_content ã§ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å®‰å…¨ã«å–å¾—ã™ã‚‹ã€‚

    feedparser.parse(url) ã¯å†…éƒ¨ã§ urllib ã‚’ä½¿ã„ã€TCPãƒãƒ£ãƒ³ã‚¯é–“ã§
    read timeout ãŒãƒªã‚»ãƒƒãƒˆã•ã‚Œã‚‹ãŸã‚ç„¡é™ã«ãƒãƒ³ã‚°ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
    ä»£ã‚ã‚Šã« requests + ãƒãƒ¼ãƒ‰ãƒ‡ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ã§å®‰å…¨ã«å–å¾—ã™ã‚‹ã€‚
    (GameResearch Bug #2 ã®ãƒãƒƒã‚¯ãƒãƒ¼ãƒˆ)
    """
    resp = requests.get(
        url,
        timeout=(5, 10),  # (connect, read) timeout
        headers={"User-Agent": "MathDailyDigest/1.0 (RSS Reader)"},
        stream=True,
    )
    resp.raise_for_status()

    chunks = []
    bytes_read = 0
    deadline = time.time() + FEED_READ_DEADLINE_SEC

    for chunk in resp.iter_content(chunk_size=8192):
        chunks.append(chunk)
        bytes_read += len(chunk)
        if bytes_read >= FEED_MAX_BYTES:
            log.warning(f"    âš  ã‚µã‚¤ã‚ºä¸Šé™åˆ°é” ({name}: {bytes_read} bytes)")
            break
        if time.time() > deadline:
            log.warning(f"    âš  èª­ã¿å–ã‚Šãƒ‡ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³åˆ°é” ({name}: {bytes_read} bytes)")
            break

    resp.close()
    return b"".join(chunks)


def fetch_feed(feed_cfg: dict, max_age_hours: int, global_max: int) -> list[dict]:
    """å˜ä¸€ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã—ã€æ§‹é€ åŒ–ã—ã¦è¿”ã™ã€‚"""
    url = feed_cfg["url"]
    name = feed_cfg["name"]
    per_feed_max = feed_cfg.get("max_articles", global_max)

    log.info(f"  ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ä¸­: {name}")

    try:
        content = _fetch_feed_content(url, name)
        parsed = feedparser.parse(content)
    except requests.Timeout:
        log.warning(f"  âš  æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({name})")
        return []
    except requests.RequestException as e:
        log.warning(f"  âš  ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—å¤±æ•— ({name}): {e}")
        return []
    except Exception as e:
        log.warning(f"  âš  ãƒ•ã‚£ãƒ¼ãƒ‰è§£æå¤±æ•— ({name}): {e}")
        return []

    if parsed.bozo and not parsed.entries:
        log.warning(f"  âš  ãƒ•ã‚£ãƒ¼ãƒ‰è§£æã‚¨ãƒ©ãƒ¼ ({name}): {parsed.bozo_exception}")
        return []

    cutoff = datetime.now(timezone.utc) - timedelta(hours=max_age_hours)
    articles = []

    for entry in parsed.entries:
        # æ—¥æ™‚ã®è§£æ
        pub_date = None
        for date_field in ("published", "updated", "created"):
            raw = entry.get(date_field)
            if raw:
                try:
                    pub_date = date_parser.parse(raw)
                    if pub_date.tzinfo is None:
                        pub_date = pub_date.replace(tzinfo=timezone.utc)
                    break
                except (ValueError, TypeError):
                    continue

        # æ—¥æ™‚ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæ—¥æ™‚ãŒå–ã‚Œãªã„å ´åˆã¯å«ã‚ã‚‹ï¼‰
        if pub_date and pub_date < cutoff:
            continue

        title = clean_html(entry.get("title", "ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãªã—ï¼‰"))
        if not title:
            continue

        desc_raw = entry.get("summary", "") or entry.get("description", "")
        description = truncate(clean_html(desc_raw))
        link = entry.get("link", "")
        category = extract_category(entry, name)

        articles.append({
            "title": title,
            "description": description,
            "url": link,
            "category": category,
            "source_name": name,
            "source_emoji": feed_cfg.get("emoji", "ğŸ“Œ"),
            "source_category": feed_cfg.get("category", ""),
            "published": pub_date.isoformat() if pub_date else "",
        })

        if len(articles) >= per_feed_max:
            break

    log.info(f"    â†’ {len(articles)} ä»¶å–å¾—")
    return articles


def fetch_all_feeds(config: dict) -> dict[str, list[dict]]:
    """å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã€ã‚½ãƒ¼ã‚¹åã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦è¿”ã™ã€‚"""
    schedule = config.get("schedule", {})
    max_age = schedule.get("max_age_hours", 24)
    global_max = schedule.get("max_articles_per_feed", 5)

    feeds = config.get("feeds", [])
    results: dict[str, list[dict]] = {}

    log.info(f"ğŸ“¡ {len(feeds)} ãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã‚’é–‹å§‹...")

    for feed_cfg in feeds:
        name = feed_cfg["name"]
        articles = fetch_feed(feed_cfg, max_age, global_max)
        if articles:
            results[name] = articles

    total = sum(len(v) for v in results.values())
    log.info(f"âœ… å–å¾—å®Œäº†: {len(results)} ã‚½ãƒ¼ã‚¹ / {total} è¨˜äº‹")

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase B-2: åˆ†é‡ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _apply_category_filter(
    grouped: dict[str, list[dict]],
    config: dict,
) -> dict[str, list[dict]]:
    """config ã® include_categories / exclude_categories ã«åŸºã¥ã„ã¦è¨˜äº‹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ã€‚"""
    filters = config.get("filters", {})
    include = filters.get("include_categories", [])
    exclude = filters.get("exclude_categories", [])

    if not include and not exclude:
        return grouped

    filtered: dict[str, list[dict]] = {}
    removed_count = 0

    for source_name, articles in grouped.items():
        kept = []
        for art in articles:
            cats = [c.strip() for c in art.get("category", "").split(",")]

            # include ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ: ã„ãšã‚Œã‹ã®ã‚«ãƒ†ã‚´ãƒªã«ä¸€è‡´ã™ã‚‹è¨˜äº‹ã®ã¿
            if include:
                if not any(c in include for c in cats):
                    removed_count += 1
                    continue

            # exclude ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ: ã„ãšã‚Œã‹ã®ã‚«ãƒ†ã‚´ãƒªã«ä¸€è‡´ã™ã‚‹è¨˜äº‹ã‚’é™¤å¤–
            if exclude:
                if any(c in exclude for c in cats):
                    removed_count += 1
                    continue

            kept.append(art)

        if kept:
            filtered[source_name] = kept

    if removed_count:
        log.info(f"ğŸ” åˆ†é‡ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {removed_count} ä»¶é™¤å¤–")

    return filtered


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase B-1: AIè¦ç´„ (Gemini API)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _call_gemini(api_key: str, prompt: str) -> dict | None:
    """Gemini API ã‚’å‘¼ã³å‡ºã™ï¼ˆãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰ã€‚

    ã‚¯ã‚©ãƒ¼ã‚¿è¶…é(429/503)æ™‚ã«æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•è©¦è¡Œã™ã‚‹ã€‚
    """
    for model in GEMINI_MODELS:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
        try:
            resp = requests.post(
                url,
                json={
                    "contents": [{"parts": [{"text": prompt}]}],
                    "generationConfig": {
                        "temperature": 0.3,
                        "maxOutputTokens": 2048,
                        "responseMimeType": "application/json",
                    },
                },
                timeout=30,
            )

            result = resp.json()

            # ã‚¯ã‚©ãƒ¼ã‚¿è¶…éãƒã‚§ãƒƒã‚¯
            if resp.status_code == 429 or resp.status_code == 503:
                log.warning(f"    âš  {model}: ã‚¯ã‚©ãƒ¼ã‚¿è¶…é â€” æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ")
                continue

            # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯ (candidates ãŒãªã„å ´åˆ)
            if "candidates" not in result:
                error_msg = result.get("error", {}).get("message", "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")
                if "quota" in error_msg.lower() or "rate" in error_msg.lower():
                    log.warning(f"    âš  {model}: {error_msg} â€” æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ")
                    continue
                log.warning(f"    âš  {model}: {error_msg}")
                continue

            log.info(f"    ğŸ“¡ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model}")
            return result

        except requests.RequestException as e:
            log.warning(f"    âš  {model}: é€šä¿¡ã‚¨ãƒ©ãƒ¼ ({e}) â€” æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ")
            continue

    log.error("    âŒ å…¨ãƒ¢ãƒ‡ãƒ«ã§AIè¦ç´„ã«å¤±æ•—")
    return None


def _extract_json(text: str) -> list | None:
    """Gemini ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONé…åˆ—ã‚’å®‰å…¨ã«æŠ½å‡ºã™ã‚‹ã€‚

    Gemini ã¯æ™‚ã€… markdown ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã§ãƒ©ãƒƒãƒ—ã—ãŸã‚Š
    ä½™åˆ†ãªãƒ†ã‚­ã‚¹ãƒˆã‚’ä»˜åŠ ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€å …ç‰¢ã«æŠ½å‡ºã™ã‚‹ã€‚
    """
    # ç›´æ¥ãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã‚‹
    try:
        return json.loads(text)
    except (json.JSONDecodeError, TypeError):
        pass

    # markdown ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»ã—ã¦å†è©¦è¡Œ
    cleaned = re.sub(r"```(?:json)?\s*", "", text).strip()
    cleaned = re.sub(r"```\s*$", "", cleaned).strip()
    try:
        return json.loads(cleaned)
    except (json.JSONDecodeError, TypeError):
        pass

    # ãƒ†ã‚­ã‚¹ãƒˆä¸­ã® [...] ã‚’æ­£è¦è¡¨ç¾ã§æŠ½å‡º
    match = re.search(r"\[.*\]", text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(0))
        except (json.JSONDecodeError, TypeError):
            pass

    return None


def _summarize_articles(grouped: dict[str, list[dict]]) -> None:
    """Gemini API ã§è¨˜äº‹ã‚’æ—¥æœ¬èªè¦ç´„ã™ã‚‹ (in-place)ã€‚

    ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãã§è‹±èªè¨˜äº‹ã®è¦ç´„+ç¿»è¨³ã‚’åŒæ™‚ã«è¡Œã†ã€‚
    ãƒãƒƒãƒå‡¦ç†ã§ API ã‚³ãƒ¼ãƒ«ã‚’æœ€å°åŒ–ã€‚GEMINI_API_KEY æœªè¨­å®šæ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚
    """
    api_key = os.environ.get("GEMINI_API_KEY", "")
    if not api_key:
        log.info("â„¹ï¸  GEMINI_API_KEY æœªè¨­å®š â€” AIè¦ç´„ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return

    # å…¨è¨˜äº‹ã‚’ãƒ•ãƒ©ãƒƒãƒˆã«é›†ã‚ã‚‹
    all_articles: list[dict] = []
    for articles in grouped.values():
        all_articles.extend(articles)

    if not all_articles:
        return

    log.info(f"ğŸ¤– AIè¦ç´„ã‚’é–‹å§‹... ({len(all_articles)} è¨˜äº‹)")

    # ãƒãƒƒãƒå‡¦ç†
    batch_size = GEMINI_BATCH_SIZE

    for i in range(0, len(all_articles), batch_size):
        batch = all_articles[i: i + batch_size]

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        prompt_lines = [
            "ä»¥ä¸‹ã®æ•°å­¦ãƒ»ç§‘å­¦è¨˜äº‹ã®ãƒªã‚¹ãƒˆã«ã¤ã„ã¦ã€ãã‚Œãã‚Œæ—¥æœ¬èªã§1-2è¡Œã®ç°¡æ½”ãªè¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚",
            "æ•°å­¦ç”¨èªã¯æ­£ç¢ºã«è¨³ã—ã¦ãã ã•ã„ï¼ˆä¾‹: ringâ†’ç’°, fieldâ†’ä½“, categoryâ†’åœ, manifoldâ†’å¤šæ§˜ä½“ï¼‰ã€‚",
            'JSONã®é…åˆ—å½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚å„è¦ç´ ã¯ {"index": ç•ªå·, "summary": "è¦ç´„"} ã®å½¢å¼ã§ã™ã€‚',
            "",
        ]
        for idx, art in enumerate(batch):
            prompt_lines.append(
                f"[{idx}] ã‚¿ã‚¤ãƒˆãƒ«: {art['title']}"
                f"\n    èª¬æ˜: {art['description'][:300]}"
                f"\n    åˆ†é‡: {art.get('category', 'N/A')}"
            )

        prompt = "\n".join(prompt_lines)

        try:
            result = _call_gemini(api_key, prompt)
            if result is None:
                continue

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æï¼ˆå …ç‰¢ãªJSONæŠ½å‡ºï¼‰
            text = result["candidates"][0]["content"]["parts"][0]["text"]
            summaries = _extract_json(text)

            if summaries is None:
                log.warning(f"    âš  ãƒãƒƒãƒ {i // batch_size + 1}: JSONæŠ½å‡ºå¤±æ•—")
                log.debug(f"    Raw response: {text[:200]}")
                continue

            applied = 0
            for item in summaries:
                idx = item.get("index", -1)
                summary = item.get("summary", "")
                if 0 <= idx < len(batch) and summary:
                    batch[idx]["ai_summary"] = f"ğŸ¤– {summary}"
                    applied += 1

            log.info(f"    âœ… ãƒãƒƒãƒ {i // batch_size + 1} å®Œäº† ({applied}/{len(batch)} ä»¶)")

        except Exception as e:
            log.warning(f"    âš  AIè¦ç´„ã‚¨ãƒ©ãƒ¼ (ãƒãƒƒãƒ {i // batch_size + 1}): {e}")

    summarized = sum(1 for a in all_articles if "ai_summary" in a)
    log.info(f"ğŸ¤– AIè¦ç´„å®Œäº†: {summarized}/{len(all_articles)} ä»¶")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Discord é€ä¿¡ (Phase A-2: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒªãƒˆãƒ©ã‚¤å¯¾å¿œ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_embed_fields(articles: list[dict]) -> list[dict]:
    """è¨˜äº‹ãƒªã‚¹ãƒˆã‚’ Discord embed ã® fields ã«å¤‰æ›ã™ã‚‹ã€‚"""
    fields = []
    for art in articles:
        cat_str = f" ({art['category']})" if art["category"] else ""
        name = f"ğŸ“ {art['title']}"
        if len(name) > 256:
            name = name[:255] + "â€¦"

        value_parts = []
        # AIè¦ç´„ãŒã‚ã‚‹å ´åˆã¯ãã¡ã‚‰ã‚’å„ªå…ˆè¡¨ç¤º
        if art.get("ai_summary"):
            value_parts.append(art["ai_summary"])
        elif art["description"]:
            value_parts.append(art["description"])
        if art["url"]:
            value_parts.append(f"ğŸ”— [è¨˜äº‹ã‚’èª­ã‚€]({art['url']})")
        if cat_str:
            value_parts.append(f"ğŸ·ï¸{cat_str}")

        value = "\n".join(value_parts) if value_parts else "â€”"
        if len(value) > DISCORD_FIELD_LIMIT:
            value = value[: DISCORD_FIELD_LIMIT - 1] + "â€¦"

        fields.append({"name": name, "value": value, "inline": False})

    return fields


def _embed_char_count(embed: dict) -> int:
    """embed å†…ã®æ–‡å­—æ•°ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆDiscord API ã®åˆ¶é™ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰ã€‚"""
    count = 0
    count += len(embed.get("title", ""))
    count += len(embed.get("description", ""))
    for field in embed.get("fields", []):
        count += len(field.get("name", ""))
        count += len(field.get("value", ""))
    footer = embed.get("footer", {})
    count += len(footer.get("text", ""))
    count += len(embed.get("author", {}).get("name", ""))
    return count


def build_discord_payloads(grouped: dict[str, list[dict]], date_str: str) -> list[dict]:
    """Discord ã«é€ä¿¡ã™ã‚‹ payload ã®ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

    Discord API ã®åˆ¶é™:
    - 1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ãŸã‚Šæœ€å¤§ 10 embed
    - 1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®embedåˆè¨ˆæ–‡å­—æ•°ãŒ 6000 æ–‡å­—ä»¥å†…
    """
    today = date_str
    embeds = []

    # ãƒ˜ãƒƒãƒ€ãƒ¼ embed
    has_ai = any(
        art.get("ai_summary")
        for arts in grouped.values()
        for art in arts
    )
    desc = "æ•°å­¦ãƒ»å°‚é–€æƒ…å ±ã®æ—¥æ¬¡ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ"
    if has_ai:
        desc += " (ğŸ¤– AIè¦ç´„ä»˜ã)"

    embeds.append({
        "title": f"ğŸ“ Math Daily Digest ({today})",
        "description": desc,
        "color": 0x4A90D9,
    })

    # ã‚«ãƒ†ã‚´ãƒªé †ã«ä¸¦ã¹ã‚‹
    category_order = ["è«–æ–‡", "ãƒ‹ãƒ¥ãƒ¼ã‚¹", "ãƒ–ãƒ­ã‚°", "YouTube"]
    category_colors = {
        "è«–æ–‡": 0xE74C3C,
        "ãƒ‹ãƒ¥ãƒ¼ã‚¹": 0x2ECC71,
        "ãƒ–ãƒ­ã‚°": 0xF39C12,
        "YouTube": 0x9B59B6,
    }

    for cat in category_order:
        sources_in_cat = {
            name: arts
            for name, arts in grouped.items()
            if arts and arts[0].get("source_category") == cat
        }
        if not sources_in_cat:
            continue

        for source_name, articles in sources_in_cat.items():
            emoji = articles[0].get("source_emoji", "ğŸ“Œ") if articles else "ğŸ“Œ"
            fields = build_embed_fields(articles)

            embed = {
                "title": f"{emoji} {source_name}",
                "color": category_colors.get(cat, 0x95A5A6),
                "fields": fields,
            }
            embeds.append(embed)

    # ãƒ•ãƒƒã‚¿ãƒ¼ embed
    total_sources = len(grouped)
    total_articles = sum(len(v) for v in grouped.values())
    embeds.append({
        "title": f"ğŸ“Š æœ¬æ—¥ã®é›†è¨ˆ: {total_sources} ã‚½ãƒ¼ã‚¹ / {total_articles} è¨˜äº‹",
        "color": 0x4A90D9,
    })

    # ã‚µã‚¤ã‚ºã¨å€‹æ•°ã®åˆ¶é™ã‚’è€ƒæ…®ã—ã¦åˆ†å‰²
    payloads = []
    current_chunk: list[dict] = []
    current_chars = 0

    for embed in embeds:
        embed_size = _embed_char_count(embed)
        would_exceed_chars = (current_chars + embed_size) > DISCORD_EMBED_LIMIT
        would_exceed_count = len(current_chunk) >= MAX_EMBEDS_PER_MESSAGE

        if current_chunk and (would_exceed_chars or would_exceed_count):
            payloads.append({"embeds": current_chunk})
            current_chunk = []
            current_chars = 0

        current_chunk.append(embed)
        current_chars += embed_size

    if current_chunk:
        payloads.append({"embeds": current_chunk})

    return payloads


def _post_webhook(url: str, payload: dict, max_retries: int = 3) -> bool:
    """Discord Webhook ã«é€ä¿¡ã™ã‚‹ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰ã€‚

    GameResearch Bug #1 ã®ãƒãƒƒã‚¯ãƒãƒ¼ãƒˆ:
    429 (Rate Limited) å¿œç­”æ™‚ã« retry_after ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã€‚
    """
    for attempt in range(max_retries):
        try:
            resp = requests.post(url, json=payload, timeout=15)
            if resp.status_code == 429:
                retry_after = resp.json().get("retry_after", 2)
                log.warning(f"    â³ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ â€” {retry_after}ç§’å¾…æ©Ÿ...")
                time.sleep(retry_after)
                continue
            if resp.status_code == 204:
                return True
            resp.raise_for_status()
            return True
        except requests.RequestException as e:
            log.error(f"    âŒ Discordé€ä¿¡ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
    return False


def send_to_discord(webhook_url: str, payloads: list[dict]) -> bool:
    """Discord Webhook ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã™ã‚‹ã€‚"""
    if not webhook_url:
        log.error("âŒ DISCORD_WEBHOOK_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False

    success = True
    for idx, payload in enumerate(payloads):
        log.info(f"  Discord é€ä¿¡ä¸­... ({idx + 1}/{len(payloads)})")
        if _post_webhook(webhook_url, payload):
            log.info(f"    âœ… é€ä¿¡æˆåŠŸ")
        else:
            log.error(f"    âŒ é€ä¿¡å¤±æ•— (ãƒªãƒˆãƒ©ã‚¤ä¸Šé™)")
            success = False

    return success


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    log.info("=" * 50)
    log.info("ğŸ“ Math Daily Digest â€” é–‹å§‹")
    log.info("=" * 50)

    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config()
    webhook_url = config.get("discord", {}).get("webhook_url", "")

    # ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—
    grouped = fetch_all_feeds(config)

    if not grouped:
        log.warning("âš  æ–°ç€è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Discord ã¸ã®é€ä¿¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        sys.exit(0)

    # Phase B-2: åˆ†é‡ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    grouped = _apply_category_filter(grouped, config)

    if not grouped:
        log.warning("âš  ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨å¾Œã€è©²å½“è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        sys.exit(0)

    # Phase B-1: AIè¦ç´„ (Gemini API)
    if config.get("schedule", {}).get("summarize", False):
        _summarize_articles(grouped)

    # Discord é€ä¿¡
    jst = timezone(timedelta(hours=9))
    today = datetime.now(jst).strftime("%Y/%m/%d")
    payloads = build_discord_payloads(grouped, today)

    log.info(f"ğŸ“¤ Discord ã«é€ä¿¡ä¸­... ({len(payloads)} ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)")
    ok = send_to_discord(webhook_url, payloads)

    if ok:
        log.info("âœ… Math Daily Digest â€” å®Œäº†")
    else:
        log.error("âŒ ä¸€éƒ¨ã®é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)


if __name__ == "__main__":
    main()
